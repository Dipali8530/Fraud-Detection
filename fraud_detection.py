# -*- coding: utf-8 -*-
"""Fraud Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y7loxv06TIAcExjLeWjR_1QnzZdwVZZn
"""

import pandas as pd
import seaborn as sb
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('/content/Fraud.csv')
df = data.copy()
df.head()

# Drop rows with NaN values in 'nameDest' column
df_cleaned = df.dropna(subset=['nameDest'])

# Apply the mask and sum the 'isFraud' column
total_fraudulent_transfers = df_cleaned[df_cleaned['nameDest'].str.startswith('M')]['isFraud'].sum()

print(total_fraudulent_transfers)

# Drop rows with NaN values in 'nameDest' column
df_cleaned = df.dropna(subset=['nameDest'])

# Filter and get the unique 'type' values for 'nameDest' starting with 'M'
types_with_name_dest_starting_M = df_cleaned[df_cleaned['nameDest'].str.startswith('M')]['type'].unique()

print(types_with_name_dest_starting_M)

# Drop rows with NaN values in 'nameDest' column
df_cleaned = df.dropna(subset=['nameDest'])

# Calculate the sum of 'oldbalanceDest' for rows where 'nameDest' starts with 'M'
total_old_balance_dest_for_M = df_cleaned[df_cleaned['nameDest'].str.startswith('M')]['oldbalanceDest'].sum()

print(total_old_balance_dest_for_M)

# Drop rows with NaN values in 'nameDest' column
df_cleaned = df.dropna(subset=['nameDest'])

# Calculate the sum of 'oldbalanceDest' for rows where 'nameDest' starts with 'M'
total_new_balance_dest_for_M = df_cleaned[df_cleaned['nameDest'].str.startswith('M')]['newbalanceDest'].sum()

print(total_new_balance_dest_for_M)

# Drop rows with NaN values in 'nameDest' column
df_cleaned = df.dropna(subset=['nameDest'])

# Use the mask to filter the DataFrame where 'nameDest' starts with 'M'
filtered_df = df_cleaned[df_cleaned['nameDest'].str.startswith('M')]

print(filtered_df)

df_1 = df[df["nameDest"].str.startswith("M") == False]
df_1.shape

df_1[df_1['nameDest']==('C1000004082')]['isFraud']

df_1.groupby('nameDest').value_counts()

df_1 = df_1.drop('isFlaggedFraud',axis=1)
df_1.columns

#Missing values
df_1.isnull().sum()

df_1.info()

#No NULL Values.
df_1.nunique()

df_1[df_1['type'].str.startswith('D')]

df_1[df_1['type'].str.endswith('N')]

df_1[df_1['type'].str.startswith('D')]['isFraud'].sum()

df_1[df_1['type'].str.endswith('N')]['isFraud'].sum()

"""We don't have any Fraud transaction for Debit type.

We don't have any Fraud transaction for CASH_IN type.

since no Fraud transaction in DEBIT and CASH_IN,So i can drop these rows
"""

df_2 = df_1[df_1["type"].str.endswith('N') == False]
df_2.shape

df_2 = df_2[df_2["type"].str.startswith('D') == False]
df_2.shape

df_1.duplicated().sum()

"""Outliers"""

sb.boxplot(x='type',y='amount', hue = 'isFraud',data=df_2)

"""There are a lot of outliers in 'TRANSFER' and 'CASH_OUT' but we cannot remove that data because many outliers are associated with Fraudulent transactions.

it is very clear from here Fraud transaction comes out from 'TRANSFER' and 'CASH_OUT' type transaction.

If remove outliers than it is removing our fraudulent main data for which we have to apply prediction.

So not removing outliers is the best.

"""

#Check multi-collinearity
plt.figure(figsize=(10,8))
sb.heatmap(df_1.corr(numeric_only= True), cmap='Reds',fmt='.4%', annot=True)
plt.title('Correlation')
plt.show()

"""From the graph,

oldbalanceorg and new balanceorig are highly correlated, so both are multi-colinear
oldbalanceDest and newbalanceDest are highly correlated, so both are multi-colinear

"""

df_2['isFraud'].value_counts().plot(kind='bar', color = 'orange')

"""Since our 99.87 data is 0 so it is high imbalanced data."""

df_2.nunique()

# Hyperparameter tuning will be difficult on full data and as the data is highly imbalanced, we will be sampling the highly frequent
# class accordingly
# df_new = df_2.sample(12000)
class1_df = df_2[df_2['isFraud']==1]
class0_df = df_2[df_2['isFraud']==0].sample(12000)
print(len(class1_df))
print(len(class0_df))
df_new = pd.concat([class0_df,class1_df]).reset_index(drop=True)
df_new['isFraud'].value_counts().plot(kind='bar', color='orange')

# Input
# X =df_2.drop(['type','isFraud','nameOrig','nameDest'],axis=1)
X =df_new.drop(['type','isFraud','nameOrig','nameDest'],axis=1)
# Target
# Y = df_2['isFraud']
Y = df_new['isFraud']
print(X.shape)
print(Y.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state= 1)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

#Standardization
from sklearn.preprocessing import StandardScaler
scale = StandardScaler()
scaled_train_data = scale.fit_transform(X_train)

scaled_test_data = scale.transform(X_test)

"""Evaluation metric

Model Building (With Hyperparameter Tuning)

Model 1

Random forest Hyperparaeter tuning
"""

from scipy.stats import randint
from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import RandomizedSearchCV
# Creating the hyperparameter grid
model_2 = RandomForestClassifier(class_weight='balanced')
param_dist = {"max_depth": [7],
              "max_features": [.5,.8],
              "min_samples_leaf":[6,7],
              "criterion": ["entropy"]}
 # Instantiating RandomizedSearchCV object
tree_cv = RandomizedSearchCV(model_2, param_dist, cv = 5,scoring='f1')
tree_cv.fit(scaled_train_data,y_train)

# Print the tuned parameters and score
print("Tuned RandomForest Parameters: {}".format(tree_cv.best_params_))
print("F1 score is {}".format(tree_cv.best_score_))

model_2_y_pred = tree_cv.predict(scaled_test_data)

print('Random Forest Classifier scores')
print(classification_report(y_test, model_2_y_pred))
print('ROC AUC Score')
print(roc_auc_score(y_test, model_2_y_pred))

m_2_conf_metric =confusion_matrix(y_test, model_2_y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=m_2_conf_metric)
print('Random Forest Confusion matrix')
disp.plot()
plt.show()

from sklearn.metrics import roc_curve
y_pred_prob_2 = tree_cv.predict_proba(scaled_test_data)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_2)
plt.plot(fpr, tpr, label='ROC curve')
plt.plot([0, 1], [0, 1], 'k--', label='Random classifier')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

"""XGBoost Classifier"""

import xgboost as xgb
params = { 'max_depth': [3],
           'learning_rate': [ 0.2, 0.3],
           'subsample': np.arange(0.5, 1.0),
           'colsample_bytree': np.arange(0.4, 1.0),
           'n_estimators': [100,200]}

xgbr = xgb.XGBClassifier(seed = 20)
xgclf = RandomizedSearchCV(estimator=xgbr,
                         param_distributions=params,
                         scoring='f1',
                         n_iter=25,
                         verbose=1)
xgclf.fit(scaled_train_data, y_train)

# Print the tuned parameters and score
print("Tuned XgBoost Parameters: {}".format(xgclf.best_params_))
print("F1 score is {}".format(xgclf.best_score_))

model_3_y_pred = xgclf.predict(scaled_test_data)
xgclf.best_estimator_.score(scaled_test_data,y_test)

print('XgBoost scores')
print(classification_report(y_test, model_3_y_pred))
print('ROC AUC Score')
print(roc_auc_score(y_test, model_3_y_pred))

m_3_conf_metric =confusion_matrix(y_test, model_3_y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=m_3_conf_metric)
print('XgBoost Confusion matrix')
disp.plot()
plt.show()

y_pred_prob_3 = xgclf.predict_proba(scaled_test_data)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_3)
plt.plot(fpr, tpr, label='ROC curve')
plt.plot([0, 1], [0, 1], 'k--', label='Logistic classifier')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

"""


feature importance"""

from sklearn.preprocessing import LabelEncoder
categorical_columns = df.select_dtypes(include=['object']).columns
for column in categorical_columns:
    label_encoder = LabelEncoder()
    encoded_values = label_encoder.fit_transform(df[column])
    df[column] = encoded_values

from sklearn.feature_selection import RFE
num_features =10
importance_scores = tree_cv.best_estimator_.feature_importances_
selected_columns_fi = X.columns[importance_scores.argsort()[-num_features:]].tolist()
feature_importance = pd.Series(importance_scores, index=X.columns)
feature_importance = feature_importance.sort_values(ascending=False)
plt.figure(figsize=(10, 6))
sb.barplot(x=feature_importance.values, y=feature_importance.index)
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.title('Feature Importance')
plt.show()

from sklearn.feature_selection import RFE
num_features =10
importance_scores = xgclf.best_estimator_.feature_importances_
selected_columns_fi = X.columns[importance_scores.argsort()[-num_features:]].tolist()
feature_importance = pd.Series(importance_scores, index=X.columns)
feature_importance = feature_importance.sort_values(ascending=False)
plt.figure(figsize=(10, 6))
sb.barplot(x=feature_importance.values, y=feature_importance.index)
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.title('Feature Importance')
plt.show()

"""From the graph of the most important features within the model, we can observe some key points: Most important features are "oldBalanceDest" and "oldBalanceOrg","amount",we can infer this from random-forest and XGBoost, as both of the model are ranking these features as very important.

**Conclusion**

Here best F1 Score for both XGBoost and Random Forest is very high, so both models are pretty good at what they do.

In a fraud detection model, F1 score is highly important because rather than predicting normal transactions correctly we want Fraud transactions to be predicted correctly. Because of highly imbalanced data we may catch the innocent and leave the culprit. The reason why F1 Score is being used instead of accuracy, because accuracy cannot handle imbalance in the data.

Answer of the questions

1)
Data Cleaning:

Handle missing values: Check for any missing values in the dataset and either impute them or drop the rows/columns with missing values, depending on the extent of missing data and the impact on the model.

Outlier detection: Identify outliers in numerical columns using appropriate techniques (e.g., z-scores, IQR, etc.) and decide whether to remove or transform them based on their relevance to the fraud detection problem.

Multi-collinearity: Check for multi-collinearity among the predictor variables and consider removing or combining correlated features to avoid redundancy in the model.

2)Fraud Detection Model:

We build a supervised machine learning model to predict fraudulent transactions based on the available features.
Since the dataset contains labeled data with "isFraud" indicating fraudulent transactions, we use classification algorithms to train the model.
Commonly used algorithms for fraud detection include Logistic Regression, Decision Trees, Random Forests, Gradient Boosting, and Neural Networks.
I used here the Random forests and Gradient Boosting.

3)Variable Selection:

To select variables for the model, we use techniques such as feature importance from tree-based models, correlation analysis, and domain knowledge.
We include relevant features that have a strong influence on predicting fraud and exclude irrelevant or highly correlated features that add noise to the model.

4)Performance Evaluation:

I used standard performance metrics like accuracy, precision, recall, F1-score, and ROC-AUC to evaluate the model.
To mitigate class imbalance (fraudulent transactions may be rare), techniques like oversampling, undersampling, or using class weights can be applied during model training.
Here I used the Hyperparameter tuning it help's in prevent overfitting, where a model performs well on the training data but poorly on unseen data. By finding the right hyperparameters, I achieved a balance between model complexity and generalization.

Hyperparameter tuning can be essential when dealing with imbalanced datasets, where certain hyperparameters can significantly impact the model's ability to handle class imbalance.

5) Key Factors Predicting Fraudulent Customers:

After training the model, I extract feature importance scores to identify the key factors that contribute most to predicting fraudulent transactions.
These key factors include transaction amount, customer balance changes (oldbalanceOrg, newbalanceOrig, oldbalanceDest, newbalanceDest), transaction type.

6) Interpretation of Key Factors:

The key factors identified make sense based on domain knowledge and common patterns associated with fraudulent transactions.
larger transaction amounts, sudden changes in customer balances, or certain transaction types may raise suspicion of fraud.

7) Prevention for Infrastructure Update:

To prevent fraud, the financial company can implement several measures:

a. Real-time monitoring: Implement a robust monitoring system to detect anomalies and patterns indicative of fraud in real-time.

b. Two-factor authentication: Enforce two-factor authentication for critical transactions.

c. Machine Learning Model: Integrate the developed fraud detection model into the infrastructure for proactive detection.

d. Regular Updates: Keep the infrastructure up-to-date with the latest security patches and protocols.

e. Employee Training: Train employees to identify and report suspicious activities.

8) Evaluating the effectiveness of the implemented prevention measures is crucial to ensure that they are providing the desired level of protection against fraud. Here are some ways to determine if the prevention measures are working effectively:

Monitoring and Reporting,False Positive Rate,Detection Rate (True Positive Rate),Precision, Feedback from Employees and Customers, Red Team Exercises,Continuous Improvement.

  It's important to remember that fraud prevention is an ongoing process, and continuous monitoring and adaptation are necessary to stay ahead of evolving fraud tactics.
"""